<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="From Indoor to Open World: Revealing the Spatial Reasoning Gap in MLLMs">
    <meta name="keywords" content="MLLMs, spatial reasoning, OpenBench, multimodal large language models">
    <title>From Indoor to Open World: Revealing the Spatial Reasoning Gap in MLLMs</title>
    
    <link rel="preload" href="https://airi-institute.github.io/assets/Inter/Inter-Medium.woff2" as="font" type="font/woff2" crossorigin>
    <link rel="preload" href="https://airi-institute.github.io/assets/Inter/Inter-Regular.woff2" as="font" type="font/woff2" crossorigin>

    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.0.0/dist/css/bootstrap.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/aos@2.3.4/dist/aos.css">
    <link rel="stylesheet" href="css/styles.css">
    <link rel="stylesheet" href="css/back-button.css">
    <link rel="stylesheet" href="css/menu.css">
    
    <script src="https://code.jquery.com/jquery-3.6.2.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/aos@2.3.4/dist/aos.js"></script>
    <script src="js/main.js" defer></script>
    <script src="js/copy.js" defer></script>
    <script src="js/back-button.js" defer></script>
    <script src="js/menu.js" defer></script>
    
    <style>
        /* --- 修正后的图片样式 (参考 All-Angles-Bench) --- */
        .image-container {
            width: 100%;
            text-align: center;
            margin: 30px auto; /* 上下留白 */
        }

        .diagram-image {
            max-width: 100%;      /* 限制最大宽度不超过容器 */
            height: auto;         /* 高度自适应 */
            display: inline-block; 
            border-radius: 8px;   /* 圆角，显得更现代 */
            box-shadow: 0 4px 10px rgba(0,0,0,0.1); /* 轻微阴影，增加层次感 */
        }

        .image-caption {
            margin-top: 12px;
            text-align: center;
            color: #555;
            font-size: 0.95rem;
            font-family: 'InterRegular', Arial, sans-serif;
        }

        /* --- 表格样式保持不变 --- */
        .table-container {
            width: 100%;
            overflow-x: auto;
            margin: 24px auto;
        }
        .table {
            width: 100%; /* 让表格占满容器 */
            border-collapse: collapse;
            margin-bottom: 1em;
            table-layout: auto;
        }
        .table th, .table td {
            border: 1px solid #ddd;
            padding: 10px;
            text-align: center;
            vertical-align: middle;
        }
        .table th {
            background-color: #f8f9fa;
            color: #333;
            font-weight: 600;
        }
        .table tr:nth-child(even) td { /* 偶数行变色，比较常见 */
            background-color: #f9f9f9;
        }
        .table caption {
            caption-side: bottom;
            text-align: center;
            color: #666666;
            margin-top: 10px;
        }

        /* 优化阅读体验的容器宽度 */
        .abstract-container, .hero .container {
            max-width: 960px !important; /* 限制正文最大宽度，防止在大屏上太宽 */
        }

        .author-name a {
            color: inherit; /* 继承父元素颜色，不显示默认蓝色 */
            text-decoration: none; /* 去掉下划线 */
            transition: color 0.2s ease;
        }
        .author-name a:hover {
            color: #007bff; /* 鼠标悬停时变蓝，提示可点击，颜色可根据喜好修改 */
            text-decoration: underline;
        }
    </style>
</head>
<body>
  <nav id="quick-access-menu">
    <ul>
      <li><a href="#introduction">Introduction</a></li>
      <li><a href="#methodology">Methodology</a></li>
      <li><a href="#results">Results</a></li>
      <li><a href="#discussion">Discussion</a></li>
    </ul>
  </nav>
  

  <header class="page-header">
    <div class="header-content">
        <h1 class="page-title">
            <span class="title-highlight">From Indoor to Open World:</span>
            <span class="title-subtitle">Revealing the Spatial Reasoning Gap in MLLMs</span>
        </h1>
    </div>
</header>


<section class="authors-section" style="margin-top: -70px;">
    <div class="container">
        <div class="authors-container" data-aos="fade-up" data-aos-duration="1000">
            <div class="author">
                <span class="author-name">
                    <a href="https://scholar.google.com/citations?user=Y1TtPL8AAAAJ&hl=en" target="_blank">Mingrui Wu</a><sup>1</sup>
                </span>
            </div>
            <div class="author">
                <span class="author-name">
                    <a href="https://scholar.google.com/citations?user=GuqoolgAAAAJ&hl=en" target="_blank">Jiaolong Yang</a><sup>3</sup>
                </span>
            </div>
            <div class="author">
                <span class="author-name">
                    <a href="https://scholar.google.com/citations?user=CkDanj8AAAAJ&hl=en" target="_blank">Zhaozhi Wang</a><sup>1</sup>
                </span>
            </div>
            <div class="author">
                <span class="author-name">
                    <a href="https://scholar.google.com/citations?user=YYH0BjEAAAAJ&hl=en" target="_blank">Marc Pollefeys</a><sup>2</sup>
                </span>
            </div>
            <div class="author">
                <span class="author-name">
                    <a href="https://scholar.google.com/citations?user=ysTmrEsAAAAJ&hl=en" target="_blank">Fangjinhua Wang</a><sup>2</sup>
                </span>
            </div>
            <div class="author">
                <span class="author-name">
                    <a href="https://scholar.google.com/citations?user=kCy8JG8AAAAJ&hl=en" target="_blank">Tong Zhang</a><sup>1,*</sup>
                </span>
            </div>
        </div>
        
        <div class="affiliations-container" data-aos="fade-up" data-aos-duration="1000" data-aos-delay="200">
          <div class="affiliations-column">
            <div class="affiliation">
              <div class="affiliation-text"><sup>1</sup>University of Chinese Academy of Sciences</div>
            </div>
            <div class="affiliation">
              <div class="affiliation-text"><sup>2</sup>ETH Zürich</div>
            </div>
            <div class="affiliation">
              <div class="affiliation-text"><sup>3</sup>Microsoft Research Asia</div>
            </div>
          </div>
        </div>
    </div>
</section>


<section class="abstract-section" id="introduction" data-aos="fade-in" data-aos-duration="1000" data-aos-delay="300">
    <div class="hero-body">
        <div class="container abstract-container">
            
            <div class="image-container" style="text-align: center; margin-bottom: 40px;">
                <img src="assets/teaser.png" class="diagram-image" alt="Comparison between VSI-Bench and OpenBench" loading="lazy">
                
                <div class="image-caption" style="text-align: justify; max-width: 100%; margin-top: 15px;">
                    <strong>Figure 1: Comparison between VSI-Bench [Yang et al. 2025] and OpenBench (ours).</strong> 
                    <strong>Top:</strong> Qualitative comparison of Gemini-2.5-Pro's responses, with and without vision inputs, on VSI-Bench and OpenBench. 
                    <strong>Bottom:</strong> Quantitative comparison of two benchmarks in terms of evaluation coverage, scale range, semantic diversity, and accuracy drop without vision inputs.
                    <br>
                    <span style="font-size: 0.9em; color: #666; display: block; margin-top: 5px;">
                        * The mean relative accuracy drop of GPT-4o on absolute distance tasks when vision inputs are disabled. A larger drop proves reliance on vision, not linguistic priors, in the unstructured layouts.
                    </span>
                </div>
            </div>

            <p class="abstract-text">
                <strong>Drawbacks of existing visual spatial benchmarks.</strong> They either focus on overly simplified qualitative reasoning or rely on domain-specific indoor data, constrained by the lack of outdoor datasets with verifiable metric ground truth.
            </p>
            
            <p class="abstract-text">
                <strong>Customized Open-world Data and Benchmark.</strong> We collected pedestrian-perspective open-world data—rare in prior datasets—comprising stereo video, LiDAR point clouds, and IMU/GPS data, which provide precise 3D spatial ground truth in open-world environments. Based on this data, we constructed OpenBench, spanning all three tiers of the spatial hierarchy we defined: relational reasoning, metric reasoning, and crucially, kinematic reasoning.
            </p>
            
            <p class="abstract-text">
                <strong>The Mirage of Progress and Linguistic Priors in Spatial Intelligence.</strong> We found that the progress in spatial intelligence observed on indoor benchmarks is largely a mirage—unable to transfer to open-world settings. Our analysis suggests that the strong linguistic priors inherent in structured indoor scenes partly explain this failure.
            </p>

        </div>
    </div>
</section>

<section class="hero" id="results" data-aos="fade-in" data-aos-duration="1000" data-aos-delay="300">
    <div class="hero-body">
        <div class="container">
            <h1 style="font-family: 'InterMedium', Arial, sans-serif;">Results</h1>
            <p style="font-size: 17px; color: #444; margin-bottom: 0.1cm; font-family: 'InterRegular', Arial, sans-serif;">
                We conducted a comprehensive evaluation of MLLMs using OpenBench, comparing both open and closed-source models. The results indicate that all models perform significantly below human-level accuracy, particularly in relational reasoning tasks. The closed-source Gemini-2.5-Pro model shows the highest performance across several tasks, while Qwen3-VL-32B leads among open-source models.
            </p>
            <div class="table-container">
                <table class="table">
                    <caption style="caption-side: bottom; text-align: center;">Table 1: Main evaluation results on OpenBench.</caption>
                    <thead>
                        <tr>
                            <th>Model</th>
                            <th>Relational Reasoning</th>
                            <th>Metric Reasoning</th>
                            <th>Kinematic Reasoning</th>
                            <th>Overall Accuracy</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <th scope="row">Gemini-2.5-Pro</th>
                            <td>75%</td>
                            <td>68%</td>
                            <td>70%</td>
                            <td>71%</td>
                        </tr>
                        <tr>
                            <th scope="row">Qwen3-VL-32B</th>
                            <td>72%</td>
                            <td>65%</td>
                            <td>67%</td>
                            <td>68%</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            
            <div class="image-container">
                <img src="assets/OpenBench-blinding-test.png" class="diagram-image" alt="Blinding test results showing reliance on linguistic priors" loading="lazy">
                <div class="image-caption">Figure 4: Blinding test results showing reliance on linguistic priors.</div>
            </div>
            
            <div class="image-container">
                <img src="assets/OpenBench-synthetic-test.png" class="diagram-image" alt="Synthetic test set results" loading="lazy">
                <div class="image-caption">Figure 5: Synthetic test set results.</div>
            </div>
        </div>
    </div>
</section>

<section class="hero" id="discussion" data-aos="fade-in" data-aos-duration="1000" data-aos-delay="300">
    <div class="hero-body">
        <div class="container">
            <h1 style="font-family: 'InterMedium', Arial, sans-serif;">Discussion</h1>
            <p style="font-size: 17px; color: #444; margin-bottom: 0.1cm; font-family: 'InterRegular', Arial, sans-serif;">
                Our findings underscore the need for a shift in the field towards developing genuine, vision-centric spatial intelligence. Current training paradigms, which rely heavily on language priors and QA-based supervision, are insufficient for fostering robust spatial reasoning capabilities. Future research should focus on reducing reliance on QA-based supervision and exploring more direct spatial supervision signals.
            </p>
        </div>
    </div>
</section>

<footer class="hero">
    <div class="hero-body">
        <div class="container" style="width: 400px; text-align: center; margin-top: 50px;">
            <a href="https://airi.net/" target="_blank">
                <img src="assets/images/AIRI&#32;-&#32;Full&#32;logo.svg" alt="AIRI - Artificial Intelligence Research Institute" style="max-width: 100%; height: 40px;">
            </a>
        </div>
    </div>
</footer>

<button id="back-to-top" title="Back to top">
  <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
    <path d="M18 15l-6-6-6 6"/>
  </svg>
</button>
</body>
</html>